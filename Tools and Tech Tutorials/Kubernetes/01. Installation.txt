192.168.0.183 - master01
192.168.0.175 - worker01
192.168.0.139 - worker02

Static IP Config
=================
sudo vi /etc/netplan/00-installer-config.yaml
# This is the network config written by 'subiquity'
network:
  version: 2
  ethernets:
    ens33:
      dhcp4: no
      addresses:
        - 192.168.0.175/24
      gateway4: 192.168.0.1
      nameservers:
        addresses:
          - 8.8.8.8
          - 8.8.4.4

sudo netplan apply

=================================
Creating a cluster with kubeadm
=================================

Make sure ports are allowed in firewall
-----------------------------------------

Protocol	Direction	Port Range		Purpose	Used By
TCP	Inbound	6443		Kubernetes 				API server	All
TCP	Inbound	2379-2380	etcd server 			client API	kube-apiserver, etcd
TCP	Inbound	10250		Kubelet API				Self, Control plane
TCP	Inbound	10259		kube-scheduler			Self
TCP	Inbound	10257		kube-controller-manager	Self

Although etcd ports are included in control plane section, you can also host your own etcd cluster externally or on custom ports.

Worker node(s)
Protocol	Direction	Port Range	Purpose	Used By
TCP	Inbound	10250		Kubelet API				Self, Control plane
TCP	Inbound	10256		kube-proxy				Self, Load balancers
TCP	Inbound	30000-32767	NodePort Services		All

Check required ports
---------------------

nc 127.0.0.1 6443 -zv -w 2

Swap configuration
------------------

sudo swapoff -a

sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

Add entries in /etc/hosts (optional but good)
==========================
sudo nano /etc/hosts
192.168.0.100 master
192.168.0.101 worker1
192.168.0.102 worker2

sudo hostnamectl set-hostname master
sudo hostnamectl set-hostname worker1
sudo hostnamectl set-hostname worker2


Enable IPv4 packet forwarding
===============================
To manually enable IPv4 packet forwarding:

# sysctl params required by setup, params persist across reboots
----------------------------------------------------------------
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
EOF

# Apply sysctl params without reboot
-------------------------------------
sudo sysctl --system

# Verify that net.ipv4.ip_forward is set to 1 with:
-----------------------------------------------------
sysctl net.ipv4.ip_forward


Install Container Runtimes (containerd)
========================================



Installing Kubeadm
--------------------
apt-get update -y
apt-get install docker.io -y
sudo usermod --groups docker --append dany
docker ps

sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg /tmp/Release.key
curl -fsSL -o /tmp/Release.key https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key
sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg /tmp/Release.key
ls -l /etc/apt/keyrings/kubernetes-apt-keyring.gpg
sudo apt update

sudo kubeadm config images list
sudo kubeadm config images pull

-----------

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.175:6443 --token zu67ma.q3c4cpa2jwjkg2oa \
        --discovery-token-ca-cert-hash sha256:61553484852632c1764719a36b3cadd57342b1c235de36aca43d47f98a32047b


mkdir -p ~/.kube
sudo cp -i /etc/kubernetes/admin.conf ~/.kube/config

=================================
Join Worker Nodes
=================================
On master node:
sudo cat /etc/kubernetes/admin.conf

On Worker node:

mkdir -p $HOME/.kube
vi $HOME/.kube/config
[paste content of /etc/kubernetes/admin.conf]

=================
Troubleshooot:
=================
journalctl --identifier kubelet

result: Failed to run kubelet" err="failed to run kubelet: misconfiguration: kubelet cgroup driver: \"systemd\" is different from docker cgroup driver: \"cgroupfs\""

Solution:
------------
docker info
result: Cgroup Driver: cgroupfs

sudo systemctl status docker.service
Unit File: /lib/systemd/system/docker.service

vi /lib/systemd/system/docker.service
ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd

sudo systemctl restart docker.service
sudo systemctl daemon-reload
docker info
sudo kubeadm reset
rm $HOME/.kube/config
sudo kubeadm init --pod-network-cidr 10.88.0.0/16

=============================

kubectl version
kubectl get namespace
kubectl get all --namespace default
kubectl get all --namespace kube-system

at this stage coredns pods will be at pending state and deployment.apps/coredns will be 0/2 ready

that is because we need to install Container Network Interface

===================================================
install Project Calico Container Network Interface
===================================================

https://github.com/projectcalico/calico

https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart

# curl https://raw.githubusercontent.com/projectcalico/calico/v3.29.3/manifests/calico.yaml -O

# kubectl apply -f calico.yaml


================================
Editing bashrc file
================================

vi ~/.bashrc

#Add below
export KUBECONFIG=$HOME/.kube/config
alias k='kubectl'
source <(kubectl completion bash)
complete -F __start_kubectl k  # for bash
# compdef k=kubectl           # for zsh

#Save

#source ~/.bashrc


sudo mkdir -p /root/.kube
sudo cp $KUBECONFIG /root/.kube/config
sudo chown root:root /root/.kube/config


sudo kubectl drain k8s-worker01 --ignore-daemonsets --delete-emptydir-data
sudo kubectl drain k8s-worker02 --ignore-daemonsets --delete-emptydir-data
sudo kubectl uncordon k8s-worker01
sudo kubectl uncordon k8s-worker02